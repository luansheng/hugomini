---
title: 岭回归学习
author: Sheng Luan
date: '2021-05-17'
slug: ridge-regression
categories:
  - 统计
tags:
  - Ridge regression
---

## 1 前言

之前一直接触的是BLUP、ssGBLUP等方法，对于岭回归、贝叶斯等方法了解甚少。最近在做抗病性状的遗传评估，需要了解和评估相关算法，先从岭回归学起。

基因组选择需要解决的问题：

* 在早期，SNP标记的数量要远大于样本量，这意味着要求解的标记效应值数量远大于观察值，会存在不唯一的解。针对这个问题，提出了[RRBLUP](https://doi.org/10.3835/plantgenome2011.08.0024)的方案。
* 但是现在问题反过来了，像奶牛分型的样本量已经达到了百万头，已经比一般的芯片，如50K要多了。现在问题是如何求解对如此庞大的样本，求解其逆矩阵。[Misztal](https://doi.org/10.3168/jds.2013-7752)等人提出了APY算法策略，即把分型个体分为核心和非核心两类，利用少量核心个体的基因组亲缘关系逆矩阵，以及非核心与核心个体间的基因组亲缘关系矩阵，直接计算得到所有分型个体的基因组亲缘关系逆矩阵，避免了直接求逆。具体可参考ssGBLUP方法的最新综述文章：[Single-Step Genomic Evaluations from Theory to Practice: Using SNP Chips and Sequence Data in BLUPF90](https://doi.org/10.3390/genes11070790)。

本文主要想了解两个问题：

* 为什么讲RRBLUP等价于GBLUP
* RRBLUP是如何解决p>n问题的

本文主要是学习RRBLUP方法，参考了[百奥云GS专栏的基因组选择之模型篇](https://www.cnblogs.com/jessepeng/p/14375487.html)的相关叙述，感觉说的特别好。

RRBLUP vs GBLUP

* RRBLUP是一种改良的最小二乘法，是间接法的代表算法。它能估计出**所有SNP的效应值**。该方法将标记效应假定为**随机效应**且服从**正态分布**，利用**线性混合效应模型**估算每个标记的效应值，然后将每个标记效应相加即得到个体的估计育种值。

* GBLUP是直接法的代表，它把**个体**作为随机效应，根据参考群体和预测群体SNP基因型信息构建的亲缘关系矩阵作为方差协方差矩阵，通过迭代法估计方差组分，进而求解线性混合效应模型获得待预测个体的估计育种值。

## 2 为什么两个模型是等价的？

考虑到Endelman等RRBLUP的文章中，公式中对G的定义跟后来的基因组亲缘关系G矩阵冲突，这里综合了[Hayes等文章](https://doi.org/10.1017/S0016672308009981)中的定义方法。

2.1 RRBLUP模型（以标记为基础的BLUP）:

$$y=ZWu+e$$
其中，$u \sim N(0,I\sigma^{2}_{u})$，u为标记等位基因的替代效应向量；W是基因型矩阵（-1,0,1等形式）；Z为观察值的关联矩阵。

假定$K=ZW$，那么标记效应的BLUP解为:
$$\hat{u}=(K'K+\lambda I)^{-1}K'y$$
其中I表示标记间是独立的，没有连锁关系。岭参数$\lambda = \sigma^{2}_{e}/\sigma^{2}_{u}$为残差方差与标记效应方差的比值。

上述公式可以根据经典的单性状BLUP模型公式推导：
$$
\begin{bmatrix}
X'X & X'K \\ 
K'X & K'K+\lambda I 
\end{bmatrix}
\begin{bmatrix}
\hat{b} \\ 
\hat{u}  
\end{bmatrix}
=
\begin{bmatrix}
X'y \\
K'y
\end{bmatrix}
$$
由于没有模型中没有考虑固定效应，因此模型可以进一步简化为
$$
(K'K+\lambda I)\hat{u} = K'y
$$
进而推导出前述公式。

2.2 常规BLUP模型

$$y=Zg+e$$
其中，$g \sim N(0,G\sigma^{2}_{g})$，g为基因型值向量；Z为观察值的关联矩阵。

注意：RRBLUP模型中标记效应指的是每个位点**某个等位基因的可替代效应**；而常规BLUP模型中基因型值指的**所有位点的基因型值之和**，即个体水平的效应。

在系谱为基础的育种值预测过程中，G指的是根据共亲系数计算得到的加性亲缘关系矩阵**A**。

从2个模型中可以看出$g=Wu$，基因型值即个体育种值的遗传方差：

$$V(g)=G\sigma^{2}_{g}=V(Wu)=WW'\sigma^{2}_{u}$$
如上所述，**这两个模型是等同的**。

因此，G矩阵也可以根据W矩阵计算得到：

$G = WW'\sigma^{2}_{g}/\sigma^{2}_{u}$。此时，G为基因组亲缘关系矩阵。

因此，$G$的另外一个特性：基于该矩阵计算得到的GEBV**等同于**利用RRBLUP模型计算得到的结果。

## 3 岭回归分析的原理

现在来看第二个问题：为什么岭回归可以解决p>n的问题。

岭回归类似于Bayesian A，假定每个SNP的效应值的相等的，符合正态分布。

当标记数量远超过观测值时，譬如有500个体的体重观测值，需要估计5万个SNP标记的效应，如果没有惩罚措施，用最小二乘估计方法，会导致**预测值标准误较大**，且**最小二乘回归估计值$\beta$不唯一**。

回归系数的计算公式为：
$$
\beta=(K'K)^{-1}K'y
$$
其中K为设计矩阵(为了跟第一部分呼应)。可以看出，$\beta$估计值依赖K。如果K为病态矩阵，那么K'K非常有可能是是奇异的或者接近奇异，直接会导致没有逆矩阵，因此会无解。

奇异（singular）的定义：该矩阵的行列式值为0。奇异表示矩阵中列存在依赖关系，并不独立。奇异只能是对应**方阵**而言的。为什么当一个矩阵的行列式（determinant）为0时，称矩阵为奇异，这里有一个比较容易理解的[解释](http://blog.sciencenet.cn/blog-315774-889594.html)，大意是这样：

> ...为何只说行列式为零的矩阵才奇异呢？这很可能是由线性方程组的解的个数引出的名词。对于系数行列式非零的情况，方程组的解是唯一的；否则，就有无穷多解。换句话说，系数行列式可能取各种值，但不管是什么值，只要不为零，相应的方程组的解一定是唯一的。但是，如果系数行列式恰巧为零，方程组的解就可以有无穷多。这样，行列式为零的矩阵就显得很“突出”、很“不一样”、很“另类”、很“奇怪”，等等。而“奇异”包含了奇怪和异端两种意思，正好用于描述这种矩阵。奇异矩阵对应的英文单词是“singular matrix”，其中“singular”有如下几种意思（参见《朗文英语辞典》）：1. a singular noun, verb, form etc is used when writing or speaking about one person or thing; 2. very great or very noticeable; 3. very unusual or strange. 显然，“singular matrix”中的“singular”对应上面的第3个意思，也带有第2个意思。

普通最小二乘估计的的核心思想是，让残差平方和（sum of squared residuals, RSS）最小：

$$
RSS =\sum^{n}_{i=1}(y_{i}-\hat{y_{i}})^{2}
$$
其中$y_{i}$和$\hat{y_{i}}$分别为观测值和模型预测值，n为观测值个数。

岭回归的方法，如前所述，加入一个惩罚项$\lambda$，让修改后的$RSS_{ridge}$最小
$$
RSS_{ridge} =\sum^{n}_{i=1}(y_{i}-\hat{y_{i}})^{2}+\lambda \sum^{p}_{j=1}\beta_{j}^{2}
$$
$\lambda$的选择是非常重要的，太小，不起作用，等同于普通最小二乘估计值；太大，回归系数估计值趋向于0，对模型的解释能力变弱。

因此在使用岭回归的过程中，需要保持方差和偏差之间的一个平衡。

$$MSE = Variance + Bias^{2} + Irreducible error$$
公式的解释，见[这里](https://www.statology.org/bias-variance-tradeoff/)。MSE，指的是mean squared error，即平均平方误差。

选择一个$\lambda$值，方差MSE持续降低，偏差增加很少，Test MSE最小，如下图所示。

![方差与偏差的平衡](https://www.statology.org/wp-content/uploads/2020/11/ridge2-768x550.png)

## 4 基于R的实例解析

参考这个[链接](https://www.statology.org/ridge-regression-in-r/)。

### 4.1 数据集

使用的数据集为集成在R中的mtcars，它来自datasets包。该数据集是Motor Trend US 杂志收集的1973到1974年期间总共32辆汽车的11个指标: 油耗及10个与设计及性能方面的指标。

```{r}
print(mtcars)
```

选择hp（总功率）作为因变量，mpg（油耗）、wt（重量）、drat（后轴比）、qsec（提速能力1/4英里需要的时间）作为自变量。

利用glmnet包来进行岭回归分析。

> Glmnet is a package that fits generalized linear and similar models via penalized maximum likelihood. The regularization path is computed for the lasso or elastic net penalty at a grid of values (on the log scale) for the regularization parameter lambda. The algorithm is extremely fast, and can exploit sparsity in the input matrix x.

该包要求因变量为向量，自变量为datamatrix格式。

```{r}
library(glmnet)
y = mtcars$hp

x <- data.matrix(mtcars[,c("mpg","wt","drat","qsec")])
```

### 4.2 拟合岭回归模型

使用glmnet()函数拟合岭回归模型。需要注意，alpha值设置为0。

不同alpha值对应的处理方法：

* 0 岭回归 Ridge regression
* 1 拉索回归 Lasso regression
* 0-1 弹性网络 Elastic net

此外，岭回归需要对数据进行归一化处理，即平均值为0，方差为1。glmnet包会自动进行标准化处理。

```{r}
ridge_reg_model <- glmnet(x, y, alpha = 0)
summary(ridge_reg_model)
```

### 4.3 选择优化的$\lambda$值

通过K折交叉验证方法，计算test MSE。汇总不同$\lambda$值对应的test MSE，选择最小test MSE对应的$\lambda$值。

cv.glmnet()函数可以执行多折交叉验证，如5折或者10折等。本文执行10折交叉验证

```{r}
cv_model <- cv.glmnet(x, y, alpha = 0, nfolds = 10)
best_lambda_s <- cv_model$lambda.min

```

画出$\lambda$值对应的test MSE，如下图所示。最优$\lambda$值对应图中x轴的log(best_lambda_s)=`r log(best_lambda_s)`位置。

```{r SelectLambda, fig.cap="lambda值与test MSE的对应关系图", fig.align='center'}
plot(cv_model)
```

### 4.4 拟合最优模型

根据确定的$\lambda$值，重新拟合模型。

```{r}
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda_s)
coef(best_model)
```
根据计算出来的回归系数，建立对总功率hp的预测模型:

hp = `r coef(best_model)[1]` + `r coef(best_model)[2]`mpg + `r coef(best_model)[3]`wt + `r coef(best_model)[4]`drat + `r coef(best_model)[5]`qsec

这样就可以根据这四个指标预测不同类型摩托的总功率了。

画一个回归系数随着$\lambda$值变化的轨迹图，可以看到，随着$\lambda$值的不断变大，最终所有的回归系数归零。**这就是为什么叫岭（脊）回归的原因？！**
```{r BetaLambda, fig.cap= "$\\lambda$值与回归系数的对应关系", fig.align='center'}
library(latex2exp)
plot(ridge_reg_model, xvar = "lambda", xlab=TeX('$Log(\\lambda)'))
```

还剩下一个问题，为什么在RR-BLUP算法中，直接设定$\lambda = \sigma^{2}_{e}/\sigma^{2}_{u}$，即残差方差与标记效应方差的比值?